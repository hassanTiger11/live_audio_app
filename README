# ğŸ™ï¸ Real-Time Speech Transcription Web App (v1)

A **browser-based, real-time speech transcription application** built with **Next.js** that listens to your **microphone**, streams audio continuously, and transcribes speech live using **OpenAIâ€™s streaming / realtime STT**.

This repository is intentionally **v1-scoped, conservative, and production-realistic**, optimized for **low latency**, **correctness**, and **AI-assisted development (Cursor)**.

---

## ğŸš€ Objective

Build a web app that:

- Captures **microphone audio only**
- Streams audio **continuously (no file recording)**
- Produces **live partial + final transcripts**
- Uses **OpenAI streaming STT**
- Runs entirely in the browser (no desktop app)
- Prioritizes **low latency (<700ms)**

---

## ğŸ§­ Explicit v1 Scope

### âœ… Supported
- Desktop **Chromium-based browsers**:
  - Chrome
  - Edge
  - Brave
- Single speaker
- Mic input only
- Live transcription

### âŒ Not Supported (by design)
- Safari (desktop & iOS)
- Mobile browsers
- System audio
- Meeting bots / integrations
- Offline transcription
- Multi-speaker diarization
- Local VAD (Voice Activity Detection)

> v1 intentionally trades coverage for **stability and simplicity**.

---

## ğŸ—ï¸ Architecture Overview
```
[ User Clicks Start ]
â†“
[ AudioContext.resume() ]
â†“
[ getUserMedia (mic) ]
â†“
[ AudioWorkletProcessor ]
â†“
[ Float32 audio frames ]
â†“
[ PCM16 Encoder ]
â†“
[ WebSocket â†’ OpenAI STT ]
â†“
[ Partial / Final Transcripts ]
â†“
[ React UI ]
```
---

Key principles:
- **No MediaRecorder**
- **No WAV / blobs**
- **No timers**
- **No manual chunking**
- **Continuous streaming only**

---

## ğŸ› ï¸ Tech Stack

### Frontend
- **Next.js (App Router)**
- TypeScript
- React

### Audio
- Web Audio API
- `AudioContext`
- `AudioWorklet`

### Networking
- Native WebSocket API

### Speech-to-Text
- **OpenAI Realtime / Streaming STT (locked for v1)**

---

## ğŸ“ Project Structure
```
/app
/page.tsx â†’ Main UI & orchestration
/api
/openai-token/route.ts â†’ Issues short-lived OpenAI token
/public
/audio-processor.js â†’ AudioWorkletProcessor
/lib
audio.ts â†’ Mic + AudioContext + Worklet lifecycle
encoder.ts â†’ Float32 â†’ PCM16 encoder
stt.ts â†’ OpenAI WebSocket client
```


> v1 intentionally avoids over-abstraction to keep Cursor reliable.

---

## ğŸ§ Audio Constraints (Strict)

These **must not be violated**:

- Sample rate: **16,000 Hz**
- Channels: **Mono**
- Encoding: **PCM16**
- Audio source: **Microphone only**

âŒ Do NOT use:
- `MediaRecorder`
- WAV files
- Blob-based recording
- Timers (`setInterval`, `setTimeout`)
- Buffered audio queues

---

## ğŸ” Audio Lifecycle Management

Browser audio requires **explicit user interaction**.

Rules:
- AudioContext is created **only after clicking â€œStartâ€**
- `audioContext.resume()` must be called explicitly
- If AudioContext becomes `suspended`:
  - Stop streaming
  - Notify user
  - Require restart

---

## ğŸŒŠ Streaming & Backpressure Policy

To maintain low latency:

- **Never buffer audio**
- **Never queue frames**
- If WebSocket is slow or congested:
  - **Drop audio frames**
  - Do not delay or retry

Rationale:
- Speech-to-text prefers missing frames over delayed speech
- Prevents memory growth
- Keeps transcript aligned with real-time speech

---

## ğŸ§  Speech-to-Text (OpenAI)

### v1 Decisions
- **OpenAI is the only STT provider**
- No multi-provider abstraction
- No local VAD (OpenAI handles segmentation)

### Data Flow
- Send: binary PCM16 frames over WebSocket
- Receive: JSON messages with:
  - Partial transcripts
  - Final transcripts
  - Errors

Token handling:
- OpenAI API key **never exposed to the browser**
- Browser receives a **short-lived token** from `/api/openai-token`

---

## ğŸ–¥ï¸ UI State Model

Minimal React state:

- `finalTranscript: string`
- `partialTranscript: string`
- `isRecording: boolean`
- `error: string | null`

Rules:
- Partial transcript **replaces** the last line
- Final transcript **appends permanently**
- UI updates throttled (~80â€“100ms)

---

## âš ï¸ Failure Modes & Behavior

| Failure | Behavior |
|------|---------|
| Mic permission denied | Stop, show error |
| AudioContext suspended | Stop streaming |
| WebSocket closed | Stop transcription |
| Token expired | Require restart |
| OpenAI error | Display message |

No auto-retry in v1 â€” user must restart manually.

---

## ğŸ” Security Model

- OpenAI API key stored in `.env.local`
- Browser never sees long-lived secrets
- Next.js API route issues **ephemeral STT tokens only**

---

## ğŸ§ª Development Phases (Cursor-Friendly)

### Phase 1 â€” Audio Validation
- Mic permissions
- AudioContext lifecycle
- Log audio RMS levels

### Phase 2 â€” Streaming Skeleton
- WebSocket connection
- Send dummy audio
- Verify backpressure handling

### Phase 3 â€” OpenAI STT Integration
- Stream PCM audio
- Display partial transcripts live

### Phase 4 â€” UX Hardening
- Start / Stop logic
- Error handling
- Cleanup on stop

---

## ğŸ¤– Cursor Usage Guidelines

To avoid AI-induced bugs:

- Implement **one file at a time**
- Do NOT ask Cursor to build the full app at once
- Manually review:
  - AudioWorklet code
  - PCM encoding logic
  - WebSocket binary framing

Cursor is powerful, but **audio code needs human review**.

---

## ğŸ§­ Deferred to v2

- Safari support
- Mobile browsers
- Local Voice Activity Detection (VAD)
- Multi-STT provider support
- Transcript export & persistence
- Summarization & insights

---

## âœ… Success Criteria

The app is successful when:
- Transcription appears **while speaking**
- Latency feels near-instant
- Partial text updates smoothly
- Final text is stable and accurate
- CPU and memory usage remain low

---

## ğŸ“Œ Status

**v1 â€” In active development**

Focused on correctness, latency, and architectural clarity.

---

If youâ€™re using this with **Cursor**, start by implementing:
1. `audio.ts`
2. `audio-processor.js`
3. `encoder.ts`
4. `stt.ts`
5. `page.tsx`

One file at a time.

